{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V71UDR_j-fxy"
      },
      "source": [
        "load data and create dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BE-ptj4_rTJw"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import pandas as pd  # Import Pandas for data manipulation\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import zipfile\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
        "\n",
        "random.seed(8903324324320423)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsAsOaEGHL2a",
        "outputId": "af9b02b7-a886-4502-88f3-3058bc743d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device_name = \"cpu\"  # default device is CPU\n",
        "if torch.cuda.is_available():\n",
        "    device_name = \"cuda\"  # CUDA for NVIDIA GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    device_name = torch.device(\"mps\")  # Metal Performance Shaders for Apple M-series GPU\n",
        "device = torch.device(device_name)\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rober\\OneDrive\\Desktop\\CS\\Github Repos\\ai_img_detection\\notebooks\n",
            "c:\\Users\\rober\\OneDrive\\Desktop\\CS\\Github Repos\\ai_img_detection\n"
          ]
        }
      ],
      "source": [
        "root_dir = './data/cifake'\n",
        "print(os.getcwd())\n",
        "# Ensure working directory is project root, not /notebooks\n",
        "if Path(os.getcwd()).name == 'notebooks':\n",
        "    os.chdir(Path(os.getcwd()).parent)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-f6nXUR-oSs",
        "outputId": "d9d77af7-f8d9-43ad-db0f-f41e07c4e574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "120000 120000\n",
            "(120000, 2)\n",
            "                                image label\n",
            "0  ./data/cifake\\test\\FAKE\\0 (10).jpg  FAKE\n",
            "1   ./data/cifake\\test\\FAKE\\0 (2).jpg  FAKE\n",
            "2   ./data/cifake\\test\\FAKE\\0 (3).jpg  FAKE\n",
            "3   ./data/cifake\\test\\FAKE\\0 (4).jpg  FAKE\n",
            "4   ./data/cifake\\test\\FAKE\\0 (5).jpg  FAKE\n"
          ]
        }
      ],
      "source": [
        "# Using a list comprehension and generator expression\n",
        "file_paths = [os.path.join(dirpath, filename) for dirpath, _, filenames in os.walk(root_dir) for filename in filenames]\n",
        "labels = [os.path.basename(os.path.dirname(filepath)) for filepath in file_paths]\n",
        "\n",
        "# Print the total number of file names and labels\n",
        "print(len(file_paths), len(labels))\n",
        "\n",
        "# Create a pandas dataframe from the collected file names and labels\n",
        "df = pd.DataFrame.from_dict({\"image\": file_paths, \"label\": labels})\n",
        "print(df.shape)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pvsNIambaDi"
      },
      "source": [
        "data transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ltvtdviibYrQ",
        "outputId": "a126c957-02d1-4c9e-f8f9-03b638583c42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\rober\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Compose your transformations\n",
        "transform = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=(32,32), antialias=True),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=(32,32), antialias=True),\n",
        "    v2.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(root=f\"{root_dir}/train\", transform=transform)\n",
        "test_dataset = ImageFolder(root=f\"{root_dir}/test\", transform=test_transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset)) #TODO change\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=True,num_workers=2)\n",
        "test_loader= DataLoader(test_dataset, batch_size=100, shuffle=False,num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SFj03Doo17hf"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "  def __init__(self, patience, min_delta):\n",
        "      self.patience = patience\n",
        "      self.min_delta = min_delta\n",
        "      self.counter = 0\n",
        "      self.min_validation_loss = float('inf')\n",
        "\n",
        "  def early_stop(self, validation_loss):\n",
        "      if validation_loss < self.min_validation_loss:\n",
        "          self.min_validation_loss = validation_loss\n",
        "          self.counter = 0\n",
        "      elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "          self.counter += 1\n",
        "          if self.counter >= self.patience:\n",
        "              return True\n",
        "      return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DcIqTH0eeptz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu5(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5vII3AcjvKF4"
      },
      "outputs": [],
      "source": [
        "# Wrap your training loop with the Bottleneck profiler\n",
        "\n",
        "def train(dataloader , model , loss_func, optimizer):\n",
        "\n",
        "      model.train()\n",
        "      train_loss = []\n",
        "\n",
        "      for input, target in tqdm(dataloader):\n",
        "\n",
        "          input, target = input.to(device), target.to(device)\n",
        "          pred = model(input)\n",
        "          pred = torch.squeeze(pred)\n",
        "          loss = loss_func(pred, target.float())\n",
        "          #backpropagation\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss.append(loss.item())\n",
        "      return train_loss\n",
        "\n",
        "def validate(dataloader, model, loss_func, threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss= 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for input, target in tqdm(dataloader):\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            pred = model(input)\n",
        "            pred = torch.squeeze(pred)\n",
        "            test_loss += loss_func(pred, target.float()).item()\n",
        "            predicted = (pred >= threshold).float()\n",
        "            y_true.extend(target.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    print(f\"Validation loss: {test_loss:>8f} \\n\")\n",
        "    print(f\"Validation accuracy: {acc:>8f} \\n\")\n",
        "    print(f\"Validation F1: {f1:>8f} \\n\")\n",
        "    print(f\"Validation mcc: {mcc:>8f} \\n\")\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "def test(dataloader, model, threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        print(\"Evaluating on test set:\")\n",
        "        for input, target in tqdm(dataloader):\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            pred = model(input)\n",
        "            pred = torch.squeeze(pred)\n",
        "            predicted = (pred >= threshold).float()\n",
        "            y_true.extend(target.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred\": y_pred,\n",
        "                \"acc\": accuracy_score(y_true, y_pred),\n",
        "                \"f1\": f1_score(y_true, y_pred, average=\"micro\"),\n",
        "                \"mcc\": matthews_corrcoef(y_true, y_pred)\n",
        "            }\n",
        "\n",
        "\n",
        "def classify():\n",
        "    early_stopper = EarlyStopper(patience=1, min_delta=0.01)\n",
        "\n",
        "    input_size = 32*32\n",
        "    output_size= 1\n",
        "    ff = CustomCNN(train_loader.batch_size).to(device)\n",
        "    loss_fun = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(ff.parameters(), lr = 1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    epochs = 5\n",
        "    train_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    print(\"Epochs:\")\n",
        "    for t in tqdm(range(epochs)):\n",
        "        losses = train(train_loader, ff, loss_fun, optimizer)\n",
        "        train_loss.append(losses)\n",
        "        validation_loss = validate(val_loader, ff, loss_fun)\n",
        "        test_loss.append(validation_loss)\n",
        "        if early_stopper.early_stop(validation_loss):\n",
        "            print(\"early stop\")\n",
        "            break\n",
        "        scheduler.step()\n",
        "\n",
        "    plt.plot([i for i in range(len(train_loss))], torch.tensor(train_loss).mean(axis=1), color=\"blue\") #training loss\n",
        "    plt.plot([i for i in range(len(test_loss))], test_loss, color=\"red\") #testing loss\n",
        "    result = test(test_loader, ff)\n",
        "    print(f\"Test accuracy: {result['acc']}\")\n",
        "    print(f\"Test F1: {result['f1']}\")\n",
        "    print(f\"Test MCC: {result['mcc']}\")\n",
        "\n",
        "    conf_matrix = confusion_matrix(result[\"y_true\"], result[\"y_pred\"])\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    labels = ['Real', 'Fake']\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "CustomCNN.__init__() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mclassify\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     78\u001b[39m input_size = \u001b[32m32\u001b[39m*\u001b[32m32\u001b[39m\n\u001b[32m     79\u001b[39m output_size= \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m ff = \u001b[43mCustomCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     81\u001b[39m loss_fun = nn.BCEWithLogitsLoss()\n\u001b[32m     82\u001b[39m optimizer = torch.optim.AdamW(ff.parameters(), lr = \u001b[32m1e-3\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: CustomCNN.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "classify()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
